{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtCh/lN/F5wgwGwwE9ouMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orikopel/textnet/blob/main/text_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "!pip install -q networkx\n",
        "!pip install -q pyvis"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OdaqlkMvgB1L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network\n",
        "from nltk.corpus import stopwords\n",
        "from itertools import combinations\n",
        "from nltk.tokenize import word_tokenize\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from networkx.algorithms.community import girvan_newman"
      ],
      "metadata": {
        "id": "HOy7EWS_xD-L"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "fBin8hXryeco"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnlnxUyMArtb",
        "outputId": "4ff60fb7-94f4-4fcd-9b8c-0b0acc602cb5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "fPkR1xxeyh_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_data(df, id_col, title_col, text_col):\n",
        "    \"\"\"\n",
        "    Validates the input data.\n",
        "    \"\"\"\n",
        "\n",
        "    # filter out rows without texts\n",
        "    df = df[df[text_col].apply(lambda x: isinstance(x, str) and len(x) > 0)]\n",
        "\n",
        "    # get rid of non-letter chars\n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x) if isinstance(x, str) else x)\n",
        "\n",
        "    # drop duplicates by id\n",
        "    df = df.dropna(subset=[id_col])\n",
        "\n",
        "    # convert ids to string if needed\n",
        "    df[id_col] = df[id_col].astype(str)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Ud24ZdHJzYyP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_encode(texts, model, batch_size=32):\n",
        "    \"\"\"\n",
        "    Encodes texts in batches using the provided model.\n",
        "\n",
        "    Args:\n",
        "        texts (list): List of text data to encode.\n",
        "        model (SentenceTransformer): Model for encoding.\n",
        "        batch_size (int): Batch size for encoding.\n",
        "\n",
        "    Returns:\n",
        "        List of embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "LBnhoucHDt1b"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(df, text_col, model, batch_size=32, num_threads=4):\n",
        "    \"\"\"\n",
        "    Gets a df and a column name and returns a df with an embedding column.\n",
        "\n",
        "    Args:\n",
        "        df(DataFrame): df with text column, should also have an id and title column.\n",
        "        text_col(String): name of the text column.\n",
        "        model(SentenceTransformer): SentenceTransformer model.\n",
        "        batch_size(int): Number of texts to encode in one batch for efficiency. Default is 32.\n",
        "\n",
        "    Returns:\n",
        "        df(DataFrame): df with an embedding column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare text data as a list to avoid pandas row overhead\n",
        "    texts = df[text_col].values\n",
        "    n = len(texts)\n",
        "\n",
        "    # Split data for multithreading\n",
        "    splits = np.array_split(texts, num_threads)\n",
        "\n",
        "    # Use multithreading to process each batch\n",
        "    embeddings = []\n",
        "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        futures = [executor.submit(batch_encode, split, model, batch_size) for split in splits]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Generating embeddings in parallel\"):\n",
        "            embeddings.extend(future.result())\n",
        "\n",
        "    # Convert list of embeddings to a NumPy array and add to dataframe\n",
        "    df['embedding'] = np.array(embeddings).tolist()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "M1E45d_vvWT-"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_edges(G, data, i, similarity_matrix, threshold, id_col):\n",
        "    \"\"\"Helper function to add edges to the graph.\"\"\"\n",
        "    edges = []\n",
        "    for j in range(i + 1, len(data)):\n",
        "        score = similarity_matrix[i, j]\n",
        "        if score > threshold:\n",
        "            edges.append((data.iloc[i][id_col], data.iloc[j][id_col], float(score)))\n",
        "    return edges"
      ],
      "metadata": {
        "id": "4KuBOrD87l-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_similarity_nx(data, id_col, model, threshold):\n",
        "    \"\"\"\n",
        "    Creates a graph data structure with text similarity as edge score.\n",
        "\n",
        "    Args:\n",
        "        data(DataFrame): df with an embedding column.\n",
        "        id_col(String): name of the id column.\n",
        "\n",
        "    Returns:\n",
        "        G(Graph): graph data structure with text similarity as edge score.\n",
        "    \"\"\"\n",
        "\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Convert embeddings to a NumPy array\n",
        "    embeddings = np.array(data[\"embedding\"].tolist())\n",
        "\n",
        "    # Calculate pairwise similarities using a dot product and normalize\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    similarity_matrix = np.dot(embeddings, embeddings.T) / (norms * norms.T)\n",
        "\n",
        "    # Use ThreadPoolExecutor for multithreading\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for i in tqdm(range(len(data)), desc=\"Finding edges\"):\n",
        "            futures.append(executor.submit(add_edges, G, data, i, similarity_matrix, threshold, id_col))\n",
        "\n",
        "    # Collect results and add edges to the graph\n",
        "    for future in tqdm(futures, desc=\"Adding edges to graph\"):\n",
        "        edges = future.result()\n",
        "        G.add_edges_from(edges)\n",
        "\n",
        "    return G"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2IsDXDPE1EpM"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_common_keywords(titles):\n",
        "    \"\"\"\n",
        "    Extracts the most common non-stopword keywords from a list of titles.\n",
        "    Args:\n",
        "        titles (List[str]): List of titles to extract keywords from.\n",
        "    Returns:\n",
        "        common_keywords (str): The most common keyword(s) or subject(s).\n",
        "    \"\"\"\n",
        "    # Tokenize words and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for title in titles for word in word_tokenize(title) if word.isalnum()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Get the most common words\n",
        "    most_common_words = Counter(words).most_common(3)  # Top 3 common words\n",
        "    return ', '.join([word[0] for word in most_common_words])  # Return top words as a string"
      ],
      "metadata": {
        "id": "VzL5Mx3dAHKq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def G_to_net(G, id_col, title_col, text_col, data):\n",
        "    \"\"\"\n",
        "    Creates a Pyvis network from a graph data structure.\n",
        "\n",
        "    Args:\n",
        "        G(Graph): graph data structure with text similarity as edge score.\n",
        "        id_col(String): name of the id column.\n",
        "        title_col(String): name of the title column.\n",
        "        data(DataFrame): df with an embedding column.\n",
        "\n",
        "    Returns:\n",
        "        net(Network): Pyvis network.\n",
        "    \"\"\"\n",
        "\n",
        "    # find communities\n",
        "    comp = girvan_newman(G)\n",
        "    communities = next(comp)\n",
        "    colors = plt.cm.get_cmap(\"tab10\", len(communities))\n",
        "\n",
        "    # Create a Pyvis network\n",
        "    net = Network(notebook=True)\n",
        "\n",
        "    # add nodes to pyvis\n",
        "    for idx, community in tqdm(enumerate(communities), desc=\"Building pyvis graph by communities - nodes\"):\n",
        "\n",
        "        # Get titles for this community\n",
        "        titles = [data.loc[data[id_col] == node, title_col].values[0] for node in community]\n",
        "        texts = [data.loc[data[id_col] == node, title_col].values[0] for node in community]\n",
        "\n",
        "        # Create a community label (e.g., most common title)\n",
        "        community_node_id = f'community_{idx}'\n",
        "        community_title = community_node_id + \" - \" + extract_common_keywords(texts)\n",
        "\n",
        "        # Add a community node (outer circle)\n",
        "        net.add_node(community_node_id, label=community_title, title=community_title,\n",
        "                     color=f'rgba({colors(idx)[0] * 255}, {colors(idx)[1] * 255}, {colors(idx)[2] * 255}, 0.2)',\n",
        "                     size=40)  # Larger size for visibility\n",
        "\n",
        "        # Connect community node to its members\n",
        "        for node in community:\n",
        "            title_value = data.loc[data[id_col]==node, title_col].values[0]\n",
        "\n",
        "            # deal with long node titles\n",
        "            if len(title_value) > 35:\n",
        "                title_value = title_value[:35] + \"...\"\n",
        "\n",
        "            net.add_node(node, title=title_value, label=title_value) # get title from data df\n",
        "            net.nodes[-1]['color'] = f'rgba({colors(idx)[0] * 255}, {colors(idx)[1] * 255}, {colors(idx)[2] * 255}, 0.7)'\n",
        "\n",
        "            net.add_edge(community_node_id, node, color='rgba(0, 0, 0, 0)', value=0)  # Invisible edges for connection\n",
        "\n",
        "    # add edges to pyvis\n",
        "    for u, v, val in tqdm(G.edges(data=True), desc=\"Building pyvis graph by communities - edges\"):\n",
        "        net.add_edge(u, v, value=val['score'] * 10, label=val['score'])  # Scale scores for edge visibility\n",
        "\n",
        "    net.set_options(\"\"\"\n",
        "        var options = {\n",
        "        \"nodes\": {\n",
        "            \"font\": {\n",
        "            \"size\": 14\n",
        "            }\n",
        "        },\n",
        "        \"edges\": {\n",
        "            \"smooth\": {\n",
        "            \"type\": \"continuous\"\n",
        "            }\n",
        "        },\n",
        "        \"physics\": {\n",
        "            \"enabled\": true\n",
        "        }\n",
        "        }\n",
        "        \"\"\")\n",
        "\n",
        "    # Show the network\n",
        "    return net"
      ],
      "metadata": {
        "id": "M_8GVQbW1Xr5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_with_header(save_path, net, html_header):\n",
        "    \"\"\"\n",
        "    Creates an html page with a header and a network visualization.\n",
        "\n",
        "    Args:\n",
        "        save_path(String): path to save the html page.\n",
        "        net(Network): Pyvis network.\n",
        "        html_header(String): html header.\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine the header with the network output\n",
        "    with open(save_path, \"w\") as f:\n",
        "        f.write(html_header)\n",
        "        f.write(net.generate_html())  # Include the network visualization\n",
        "        f.write(\"</body></html>\")  # Close the HTML tags\n"
      ],
      "metadata": {
        "id": "7iUY4dtuxLvo"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "C6ZvAx2v1Z_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Code"
      ],
      "metadata": {
        "id": "bXE0f02qwTXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df2net(data_path, save_path, id_col, title_col, text_col, html_header, threshold, lang):\n",
        "    \"\"\"\n",
        "    Creates a network visualization from a dataframe. Combines all above functions.\n",
        "\n",
        "    Args:\n",
        "        data_path(String): path to the dataframe.\n",
        "        save_path(String): path to save the html page.\n",
        "        id_col(String): column name of the unique identifier.\n",
        "        title_col(String): column name of the title.\n",
        "        text_col(String): column name of the text.\n",
        "        html_header(String): header of the html page.\n",
        "        lang(String): language of the text. Used for choosing the right model.\n",
        "        threshold(Float): threshold for the edge score.\n",
        "    \"\"\"\n",
        "\n",
        "    # dict for choosing a model for each optional language\n",
        "    language_models = {\"eng\": \"all-MiniLM-L6-v2\", \"heb\": \"imvladikon/sentence-transformers-alephbert\"}\n",
        "\n",
        "    # choose the right model for user input language\n",
        "    model = SentenceTransformer(language_models[lang])\n",
        "    print(f\"1 - Selected model: {language_models[lang]}\")\n",
        "\n",
        "    # read the dataset and create embedding column\n",
        "    data = pd.read_csv(data_path, encoding='latin1').head(500)\n",
        "    data = validate_data(data, id_col, title_col, text_col)\n",
        "    data = generate_embeddings(data, text_col, model)\n",
        "    print(\"2 - Created embeddings for the data\")\n",
        "\n",
        "    # create a graph data structure with text similarity as edge score\n",
        "    G = create_similarity_nx(data, id_col, model, threshold)\n",
        "    print(\"3 - Created graph data structure with text similarity as edge score\")\n",
        "\n",
        "    # convert the nx graph to a net graph\n",
        "    net = G_to_net(G, id_col, title_col, text_col, data)\n",
        "    print(\"4 - Created pyvis network\")\n",
        "\n",
        "    # save the graph as an html page\n",
        "    create_with_header(save_path, net, html_header)\n",
        "    print(\"5 - Saved the graph as an html page\")"
      ],
      "metadata": {
        "id": "rPwKj99ry_Bq"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2net(\"McDonald_s_Reviews.csv\", \"mcdonalds.html\", \"reviewer_id\", \"review\", \"review\", html_header, 0.7, \"eng\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QA4MM4iLz-Bu",
        "outputId": "ca6f27b8-5170-46e7-e570-1e2279de4f7b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 - Selected model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings in parallel: 100%|██████████| 4/4 [00:21<00:00,  5.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 - Created embeddings for the data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating nx graph: 100%|██████████| 500/500 [00:00<00:00, 3286.05it/s]\n",
            "<ipython-input-61-ee41f4c35545>:18: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  colors = plt.cm.get_cmap(\"tab10\", len(communities))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 - Created graph data structure with text similarity as edge score\n",
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building pyvis graph by communities - nodes: 16it [00:00, 51.08it/s]\n",
            "Building pyvis graph by communities - edges: 100%|██████████| 165/165 [00:00<00:00, 15906.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 - Created pyvis network\n",
            "5 - Saved the graph as an html page\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "LGhVCXtqwWX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GUI"
      ],
      "metadata": {
        "id": "kLquL1FXFYDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html_header = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Similarity Relations in Mcdonalds Reviews</title>\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            margin: 20px;\n",
        "        }\n",
        "        .container {\n",
        "            display: flex;\n",
        "            flex-direction: row; /* Arrange children in a row */\n",
        "        }\n",
        "        .item {\n",
        "            margin-right: 20px; /* Spacing between items */\n",
        "        }\n",
        "    </style>\n",
        "    <style>\n",
        "        h1 {\n",
        "            text-align: center;\n",
        "            font-size: 36px;\n",
        "            margin: 20px 0;\n",
        "        }\n",
        "        p {\n",
        "            text-align: center;\n",
        "            font-size: 18px;\n",
        "            margin: 0 0 20px 0;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>The Data</h1>\n",
        "    <p>Over 33K McDonalds Reviews</p>\n",
        "    <p></p>\n",
        "    <h1>Similarity</h1>\n",
        "    <p>The similarity was calculated using SBERT embeddings and cosine similarity</p>\n",
        "</p>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MoVerAjoTap1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4cSQfhsMFS2R",
        "outputId": "1e158245-c62f-4cf8-bcfc-4f8abd7cf8bd"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import streamlit.components.v1 as components"
      ],
      "metadata": {
        "id": "LQtU8STUFPUf"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.title(\"Pyvis Graph with Streamlit\")\n",
        "\n",
        "# Generate the Pyvis HTML file\n",
        "path = \"mcdonalds.html\"\n",
        "\n",
        "# Open the HTML file and read its content\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "# Display the HTML content in the Streamlit app\n",
        "components.html(html_content, height=600)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2453DfpSFXEC",
        "outputId": "dd45e82e-f863-4e87-b354-0392e3728f32"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-25 00:32:19.809 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-25 00:32:20.006 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-10-25 00:32:20.012 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-25 00:32:20.016 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-25 00:32:20.053 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bny_Us8WHR7d",
        "outputId": "87410402-4cfd-46c3-ac4b-4e9f10cb25d0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.215.12:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}